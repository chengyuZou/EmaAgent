from typing import Dict, Any, List, Optional
from ..base import BaseTool
from pathlib import Path
import logging
import os
import io

# å¼•å…¥å¿…è¦çš„åˆ†æåº“
# æ³¨æ„ï¼šä½ éœ€è¦ç¡®ä¿ç¯å¢ƒå®‰è£…äº†: pymupdf, python-docx, python-pptx, pandas, openpyxl
import fitz  # PyMuPDF
import pandas as pd
try:
    import docx
except ImportError:
    docx = None
try:
    from pptx import Presentation
except ImportError:
    Presentation = None

from tools.base import ToolResult, ToolFailure

from utils.logger import logger

class DocumentAnalyzerTool(BaseTool):
    """
    å…¨èƒ½æ–‡ä»¶åˆ†æå·¥å…· (æ”¯æŒ PDF, Word, Excel, CSV, PPT)
    åŸºäº 'Lunisia' æ–‡ä»¶å¤„ç†èƒ½åŠ›å¢å¼ºï¼Œæ”¯æŒæ·±åº¦å†…å®¹æå–ä¸ç»Ÿè®¡åˆ†æã€‚
    """

    name: str = "analyze_document"
    description: str = (
        "ğŸ” ä¸“ä¸šæ–‡ä»¶å†…å®¹åˆ†æå·¥å…·ã€‚ç”¨äºè¯»å–å’Œæ·±åº¦åˆ†æå„ç±»æ–‡æ¡£ï¼Œæ”¯æŒ PDFã€Wordã€Excelã€CSVã€PPT ç­‰æ ¼å¼ã€‚"
        "è¿”å›å®Œæ•´æ–‡æœ¬å†…å®¹ã€å…ƒæ•°æ®ã€ç»“æ„åŒ–æ•°æ®ç»Ÿè®¡ã€‚ä»»ä½•éœ€è¦æŸ¥çœ‹æˆ–åˆ†ææ–‡ä»¶å†…å®¹çš„åœºæ™¯éƒ½åº”ä½¿ç”¨æ­¤å·¥å…·ã€‚"
    )
    parameters: dict = {
        "type": "object",
        "properties": {
            "file_path": {
                "type": "string",
                "description": "æ–‡ä»¶çš„æœ¬åœ°ç»å¯¹è·¯å¾„"
            },
            "preview_length": {
                "type": "integer",
                "description": "æ–‡æœ¬é¢„è§ˆæˆªå–çš„å­—ç¬¦é•¿åº¦ï¼Œé»˜è®¤ä¸º 10000",
                "default": 10000
            }
        },
        "required": ["file_path"]
    }

    async def execute(self, file_path: str, preview_length: int = 10000) -> ToolResult:
        path = Path(file_path)
        if not path.exists():
            return ToolFailure(error=f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        suffix = path.suffix.lower()
        file_name = path.name

        try:
            # æ ¹æ®åç¼€åˆ†å‘å¤„ç†é€»è¾‘
            if suffix == ".pdf":
                result = self._analyze_pdf(str(path))
            elif suffix in [".docx", ".doc"]:
                result = self._analyze_docx(str(path))
            elif suffix in [".pptx", ".ppt"]:
                result = self._analyze_pptx(str(path))
            elif suffix in [".csv", ".xlsx", ".xls"]:
                result = self._analyze_table(str(path), suffix)
            else:
                return ToolFailure(error=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}\nè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å¹¶é‡æ–°ä¸Šä¼ ã€‚\næ”¯æŒçš„æ ¼å¼åŒ…æ‹¬: PDF, Word (.docx), PowerPoint (.pptx), Excel (.xlsx, .xls), CSV.")

            # æ„é€ è¿”å›ç»™ LLM çš„æœ€ç»ˆç»“æœ
            # System éƒ¨åˆ†æ”¾ç½®å…ƒæ•°æ®å’Œç»“æ„åŒ–åˆ†æï¼ŒOutput æ”¾ç½®å…·ä½“æ–‡æœ¬å†…å®¹
            metadata_str = "\n".join([f"- {k}: {v}" for k, v in result['metadata'].items()])
            analysis_str = result.get('analysis', 'æ— é¢å¤–åˆ†æ')
            
            full_content = result['content']
            # å¦‚æœå†…å®¹è¿‡é•¿ï¼Œè¿›è¡Œæˆªæ–­å¤„ç†ï¼Œä½†åœ¨ system ä¸­æç¤º
            display_content = full_content[:preview_length]
            if len(full_content) > preview_length:
                display_content += f"\n\n[...å‰©ä½™å†…å®¹å·²æˆªæ–­ï¼Œæ€»é•¿åº¦ {len(full_content)} å­—ç¬¦...]"

            system_msg = (
                f"æ–‡ä»¶åˆ†ææŠ¥å‘Š: {file_name}\n"
                f"ã€å…ƒæ•°æ®ã€‘\n{metadata_str}\n"
                f"ã€æ™ºèƒ½åˆ†æã€‘\n{analysis_str}"
            )

            return ToolResult(output=display_content, system=system_msg)

        except Exception as e:
            logger.error(f"æ–‡ä»¶åˆ†æå¤±è´¥: {e}", exc_info=True)
            return ToolFailure(error=f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

    def _analyze_pdf(self, file_path: str) -> Dict[str, Any]:
        """PDFæ·±åº¦åˆ†æ (å‚è€ƒ file_analysis_tool.PDFAnalyzer)"""
        doc = fitz.open(file_path)
        content_parts = []
        
        metadata = {
            "type": "PDF Document",
            "page_count": doc.page_count,
            "title": doc.metadata.get("title", ""),
            "author": doc.metadata.get("author", ""),
            "creation_date": doc.metadata.get("creationDate", ""),
        }

        # æå–æ–‡æœ¬
        for page_num, page in enumerate(doc):
            text = page.get_text()
            if text.strip():
                content_parts.append(f"--- ç¬¬ {page_num + 1} é¡µ ---\n{text}")
        
        full_text = "\n".join(content_parts)
        
        # ç®€å•åˆ†æï¼šæ£€æµ‹æ˜¯å¦æœ‰è¡¨æ ¼æˆ–ä»£ç ç‰¹å¾
        analysis = []
        if "Table" in full_text or "è¡¨" in full_text:
            analysis.append("æ–‡æ¡£å¯èƒ½åŒ…å«è¡¨æ ¼æ•°æ®ã€‚")
        if len(full_text) > 0:
            analysis.append(f"æœ‰æ•ˆæ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦ã€‚")
        
        doc.close()
        return {
            "metadata": metadata,
            "content": full_text,
            "analysis": "\n".join(analysis)
        }

    def _analyze_docx(self, file_path: str) -> Dict[str, Any]:
        """Wordæ·±åº¦åˆ†æ (å‚è€ƒ file_analysis_tool.DocxAnalyzer)"""
        if docx is None:
            raise ImportError("è¯·å®‰è£… python-docx ä»¥åˆ†æ Word æ–‡æ¡£")
            
        doc = docx.Document(file_path)
        core_props = doc.core_properties
        
        metadata = {
            "type": "Word Document",
            "title": core_props.title or "Unknown",
            "author": core_props.author or "Unknown",
            "paragraph_count": len(doc.paragraphs),
            "table_count": len(doc.tables),
            "modified": str(core_props.modified) if core_props.modified else ""
        }

        content_parts = []
        
        # æå–æ®µè½
        for para in doc.paragraphs:
            if para.text.strip():
                content_parts.append(para.text)
        
        # æå–è¡¨æ ¼å†…å®¹ (è¿™æ˜¯å‚è€ƒæ–‡ä»¶ä¸­éå¸¸æœ‰ç”¨çš„åŠŸèƒ½)
        if doc.tables:
            content_parts.append("\n--- æ–‡æ¡£å†…è¡¨æ ¼æ•°æ® ---")
            for i, table in enumerate(doc.tables):
                content_parts.append(f"[è¡¨æ ¼ {i+1}]")
                for row in table.rows:
                    row_text = " | ".join([cell.text.strip() for cell in row.cells])
                    content_parts.append(row_text)

        full_text = "\n\n".join(content_parts)
        
        # ç»“æ„åˆ†æ
        analysis = []
        if metadata['table_count'] > 0:
            analysis.append(f"åŒ…å« {metadata['table_count']} ä¸ªè¡¨æ ¼ï¼Œæ¶‰åŠç»“æ„åŒ–æ•°æ®ã€‚")
        
        # ç®€å•çš„æ ‡é¢˜æ£€æµ‹
        headings = sum(1 for p in doc.paragraphs if p.style.name.startswith('Heading'))
        if headings > 0:
            analysis.append(f"æ£€æµ‹åˆ° {headings} ä¸ªæ ‡é¢˜å±‚çº§ï¼Œæ–‡æ¡£ç»“æ„æ¸…æ™°ã€‚")

        return {
            "metadata": metadata,
            "content": full_text,
            "analysis": "\n".join(analysis)
        }

    def _analyze_pptx(self, file_path: str) -> Dict[str, Any]:
        """PPTXåˆ†æ"""
        if Presentation is None:
            raise ImportError("è¯·å®‰è£… python-pptx ä»¥åˆ†æ PPT æ–‡æ¡£")

        prs = Presentation(file_path)
        metadata = {
            "type": "PowerPoint Presentation",
            "slide_count": len(prs.slides)
        }
        
        content_parts = []
        for i, slide in enumerate(prs.slides):
            slide_text = []
            # æå–æ ‡é¢˜
            if slide.shapes.title:
                slide_text.append(f"Title: {slide.shapes.title.text}")
            
            # æå–æ–‡æœ¬æ¡†å†…å®¹
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape != slide.shapes.title:
                    if shape.text.strip():
                        slide_text.append(shape.text)
            
            content_parts.append(f"--- Slide {i+1} ---\n" + "\n".join(slide_text))

        return {
            "metadata": metadata,
            "content": "\n\n".join(content_parts),
            "analysis": f"å…±åŒ…å« {len(prs.slides)} å¼ å¹»ç¯ç‰‡ã€‚"
        }

    def _analyze_table(self, file_path: str, suffix: str) -> Dict[str, Any]:
        """è¡¨æ ¼æ•°æ®åˆ†æ (å‚è€ƒ file_analysis_tool.TableAnalyzer)"""
        if suffix == '.csv':
            df = pd.read_csv(file_path)
        else:
            df = pd.read_excel(file_path)
            
        rows, cols = df.shape
        
        metadata = {
            "type": "Table Data (CSV/Excel)",
            "rows": rows,
            "columns": cols,
            "column_names": df.columns.tolist(),
            "memory_usage": f"{df.memory_usage(deep=True).sum() / 1024:.2f} KB"
        }

        # ç”Ÿæˆæ•°æ®é¢„è§ˆï¼ˆMarkdown æ ¼å¼ï¼‰
        # é™åˆ¶è¡Œæ•°ä»¥é¿å… Token çˆ†ç‚¸
        preview_rows = min(rows, 20) 
        content = f"å‰ {preview_rows} è¡Œæ•°æ®é¢„è§ˆ:\n"
        content += df.head(preview_rows).to_markdown(index=False)

        # æ·±åº¦ç»Ÿè®¡åˆ†æ (å‚è€ƒ Lunisia çš„ TableAnalyzer)
        analysis_parts = []
        
        # 1. ç¼ºå¤±å€¼ç»Ÿè®¡
        missing = df.isnull().sum()
        if missing.sum() > 0:
            missing_cols = missing[missing > 0]
            missing_info = ", ".join([f"{col}({val})" for col, val in missing_cols.items()])
            analysis_parts.append(f"âš ï¸ ç¼ºå¤±å€¼æ£€æµ‹: {missing_info}")
        else:
            analysis_parts.append("âœ… æ•°æ®å®Œæ•´ï¼ˆæ— ç¼ºå¤±å€¼ï¼‰ã€‚")

        # 2. æ•°æ®ç±»å‹æ¨æ–­
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        if numeric_cols:
            analysis_parts.append(f"ğŸ“ˆ æ•°å€¼åˆ— ({len(numeric_cols)}ä¸ª): {', '.join(numeric_cols[:5])}...")
            # ç®€å•çš„æ•°å€¼ç»Ÿè®¡
            stats = df[numeric_cols].describe().to_markdown()
            content += f"\n\næ•°å€¼åˆ—ç»Ÿè®¡æè¿°:\n{stats}"

        # 3. æ—¶é—´åˆ—æ£€æµ‹
        time_candidates = [col for col in df.columns if 'date' in str(col).lower() or 'time' in str(col).lower()]
        if time_candidates:
            analysis_parts.append(f"â° å¯èƒ½çš„æ—¶é—´åˆ—: {', '.join(time_candidates)}")

        return {
            "metadata": metadata,
            "content": content,
            "analysis": "\n".join(analysis_parts)
        }