"""
æ–°é—»èšåˆæœåŠ¡æ¨¡å—

è¯¥æ¨¡å—è´Ÿè´£å¤šæ¥æºæŠ“å– ç»“æœæ¸…æ´— åå¥½åŠ æƒ ç»“æœå»é‡ ä¸ç¼“å­˜
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import math
import random
import re
from datetime import datetime, timedelta
from threading import Lock
from typing import Dict, List, Optional, Sequence, Set, Tuple

import httpx
from bs4 import BeautifulSoup

DEFAULT_QUERY = "é­”æ³•å°‘å¥³çš„é­”å¥³å®¡åˆ¤"

# çˆ¬å–å†…å®¹ä¸Šé™æ§åˆ¶åœ¨ 50-200 ä¹‹é—´ é¿å…è¿‡åº¦æŠ“å–
MIN_LIMIT = 50
MAX_LIMIT = 200

CHARACTERS = {
    "Ema": {"cn": "æ¨±ç¾½è‰¾ç›", "jp": "æ¡œç¾½ã‚¨ãƒ", "short": ["è‰¾ç›", "ã‚¨ãƒ"]},
    "Hiro": {"cn": "äºŒé˜¶å ‚å¸Œç½—", "jp": "äºŒéš å ‚ãƒ’ãƒ­", "short": ["å¸Œç½—", "ãƒ’ãƒ­"]},
    "Meruru": {"cn": "å†°ä¸Šæ¢…éœ²éœ²", "jp": "æ°´ä¸Šãƒ¡ãƒ«ãƒ«", "short": ["æ¢…éœ²éœ²", "ãƒ¡ãƒ«ãƒ«"]},
    "Milia": {"cn": "ä½ä¼¯ç±³è‰äºš", "jp": "ä½ä¼¯ãƒŸãƒªã‚¢", "short": ["ç±³è‰äºš", "ãƒŸãƒªã‚¢"]},
    "Hanna": {"cn": "è¿œé‡æ±‰å¨œ", "jp": "é é‡ãƒãƒ³ãƒŠ", "short": ["æ±‰å¨œ", "ãƒãƒ³ãƒŠ"]},
    "Coco": {"cn": "æ³½åº¦å¯å¯", "jp": "ã‚»ã‚½ãƒˆã‚³ã‚³", "short": ["å¯å¯", "ã‚³ã‚³"]},
    "Margo": {"cn": "å®ç”Ÿç›æ ¼", "jp": "å®ç”Ÿãƒãƒ«ã‚³", "short": ["ç›æ ¼", "ãƒãƒ«ã‚³"]},
    "Sherry": {"cn": "æ©˜é›ªè‰", "jp": "æ©˜ã‚·ã‚§ãƒªãƒ¼", "short": ["é›ªè‰", "ã‚·ã‚§ãƒªãƒ¼"]},
    "Leia": {"cn": "è²è§è•¾é›…", "jp": "è“®è¦‹ãƒ¬ã‚¤ã‚¢", "short": ["è•¾é›…", "ãƒ¬ã‚¤ã‚¢"]},
    "AnAn": {"cn": "å¤ç›®å®‰å®‰", "jp": "å¤ç›®ã‚¢ãƒ³ã‚¢ãƒ³", "short": ["å®‰å®‰", "ã‚¢ãƒ³ã‚¢ãƒ³"]},
    "Noah": {"cn": "åŸå´è¯ºäºš", "jp": "åŸå´ãƒã‚¢", "short": ["è¯ºäºš", "ãƒã‚¢"]},
    "Nanoka": {"cn": "é»‘éƒ¨å¥ˆå¶é¦™", "jp": "é»’éƒ¨ãƒŠãƒã‚«", "short": ["å¥ˆå¶é¦™", "ãƒŠãƒã‚«"]},
    "Alisa": {"cn": "ç´«è—¤äºšé‡Œæ²™", "jp": "ç´«è—¤ã‚¢ãƒªã‚µ", "short": ["äºšé‡Œæ²™", "ã‚¢ãƒªã‚µ"]},
    "Yuki": {"cn": "æœˆä»£é›ª", "jp": "æœˆä»£ãƒ¦ã‚­", "short": ["æœˆä»£é›ª", "ãƒ¦ã‚­"]},
    "Warden": {"cn": "å…¸ç‹±é•¿", "jp": "å…¸ç„é•·", "short": ["å…¸ç‹±é•¿"]},
    "Jailer": {"cn": "çœ‹å®ˆ", "jp": "çœ‹å®ˆ", "short": ["çœ‹å®ˆ"]},
}

# å‰©ä¸‹ä¸¤ä¸ªæš‚æ—¶ä¸æ‰“ç®—å¯ç”¨
CATEGORY_RULES = [
    {"category": "official", "keywords": ["å®˜æ–¹", "å…¬å¼", "PV", "Acacia", "é…ä¿¡", "å‘å”®", "æ›´æ–°", "å…¬å‘Š"]},
    {"category": "gameplay", "keywords": ["å®å†µ", "æµç¨‹", "æ”»ç•¥", "é€šå…³", "å…¨æµç¨‹", "æ¸¸ç©", "è¯•ç©", "å‰§æƒ…", "å½•åƒ"]},
    {"category": "fan_art", "keywords": ["äºŒåˆ›", "åŒäºº", "æ‰‹ä¹¦", "MAD", "æ’ç”»", "ç»˜ç”»", "ç”»", "ã‚¤ãƒ©ã‚¹ãƒˆ"]},
    {"category": "discussion", "keywords": ["è€ƒå¯Ÿ", "åˆ†æ", "è§£è¯´", "è®¨è®º", "è¯„æµ‹", "review", "æ„Ÿæƒ³", "ç›˜ç‚¹"]},
    {"category": "music", "keywords": ["BGM", "OST", "éŸ³ä¹", "æ­Œ", "æ›²", "ç¿»å”±"]},
    {"category": "cosplay", "keywords": ["cos", "cosplay", "ã‚³ã‚¹ãƒ—ãƒ¬"]},
]

CATEGORY_LABELS = {
    "official": "ğŸ”¶ å®˜æ–¹èµ„è®¯",
    "gameplay": "ğŸ® æ¸¸æˆå®å†µ",
    "fan_art": "ğŸ¨ åŒäººäºŒåˆ›",
    "discussion": "ğŸ’¬ è®¨è®ºè€ƒå¯Ÿ",
    "music": "ğŸµ éŸ³ä¹ç›¸å…³",
    "cosplay": "ğŸ•º Cosplay",
    "other": "ğŸ“° å…¶ä»–",
}


class NewsService:
    """
    æ–°é—»èšåˆæœåŠ¡ç±»

    è¯¥ç±»æä¾› B ç«™ ç™¾åº¦ Google æŠ“å–èƒ½åŠ›
    å¹¶åŒ…å«é‡è¯• ç¼“å­˜ åå¥½è¯åŠ æƒ ä¸ç»Ÿä¸€æ•°æ®ç»“æ„è¾“å‡º
    """

    COMMON_HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    }

    def __init__(self):
        """
        åˆå§‹åŒ–æ–°é—»æœåŠ¡ç¼“å­˜
        """
        # ç¼“å­˜ç»“æ„ key ä¸ºå‚æ•°å“ˆå¸Œ value ä¸º(ç»“æœ æ—¶é—´æˆ³)
        self._cache: Dict[str, Tuple[List[Dict], datetime]] = {}
        # çº¿ç¨‹é”ç”¨äºä¿æŠ¤ç¼“å­˜è¯»å†™ä¸€è‡´æ€§
        self._cache_lock = Lock()
        # ç¼“å­˜æ—¶æ•ˆé»˜è®¤ 10 åˆ†é’Ÿ
        self._cache_ttl = timedelta(minutes=10)

    async def fetch_news(
        self,
        source: str = "bilibili",
        query: Optional[str] = None,
        limit: int = 100,
        page: int = 1,
        preferred_sources: Optional[List[str]] = None,
        preferred_characters: Optional[List[str]] = None,
    ) -> List[Dict]:
        """
        è·å–æ–°é—»åˆ—è¡¨ä¸»å…¥å£

        è¯¥æ–¹æ³•è´Ÿè´£å‚æ•°å½’ä¸€åŒ– ç¼“å­˜å‘½ä¸­æ£€æŸ¥ åå¥½æŠ“å– åˆå¹¶æ’åº ä¸ç»“æœç¼“å­˜

        Args:
            source (str): æ•°æ®æºåç§° æ”¯æŒ bilibili baidu google
            query (Optional[str]): ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            limit (int): ç»“æœä¸Šé™
            page (int): é¡µç  ä» 1 å¼€å§‹
            preferred_sources (Optional[List[str]]): æ¥æºåå¥½åˆ—è¡¨
            preferred_characters (Optional[List[str]]): è§’è‰²åå¥½åˆ—è¡¨

        Returns:
            List[Dict]: ç»Ÿä¸€ç»“æ„æ–°é—»åˆ—è¡¨

        Examples:
            >>> rows = await svc.fetch_news(source="bilibili", query="é­”è£")
            >>> # rows is list of dict
            pass
        """

        # å½’ä¸€åŒ–åŸºç¡€æŸ¥è¯¢å‚æ•°
        source = (source or "bilibili").lower()
        page = max(page, 1)
        limit = max(MIN_LIMIT, min(MAX_LIMIT, int(limit)))
        base_search_query = self._compose_search_query(query)

        # å½’ä¸€åŒ–æ¥æºåå¥½åˆ—è¡¨
        pref_sources = self._normalize_list(preferred_sources)
        # å½’ä¸€åŒ–è§’è‰²åå¥½åˆ—è¡¨
        pref_chars = self._normalize_list(preferred_characters)

        # å…¼å®¹ source=all æˆ– source=auto åœºæ™¯
        # å½“ç»™å‡ºæ¥æºåå¥½æ—¶ä¼˜å…ˆä½¿ç”¨åå¥½ä¸­çš„é¦–ä¸ªæ¥æº
        if source in ("all", "auto") and pref_sources:
            source = pref_sources[0].lower()

        # æ„å»ºç¼“å­˜é”®å¹¶æŸ¥çœ‹å‘½ä¸­ç¼“å­˜
        cache_key = self._build_cache_key(source, base_search_query, limit, page, pref_chars + pref_sources)
        with self._cache_lock:
            cached = self._cache.get(cache_key)
            # ç¼“å­˜å‘½ä¸­ä¸”æœªè¿‡æœŸæ—¶ç›´æ¥è¿”å›ç¼“å­˜ç»“æœ é¿å…é‡å¤æŠ“å–
            if cached:
                payload, ts = cached
                if datetime.now() - ts < self._cache_ttl:
                    return payload

        # æ ¹æ®åå¥½è§’è‰²æ„å»ºåå¥½è¯åˆ—è¡¨
        preference_terms = self._build_preference_terms(pref_chars)
        # åå¥½æ¨¡å¼ä¸‹ä½¿ç”¨ 7 æ¯” 3 åˆ†é…åå¥½ä¸åŸºç¡€é…é¢
        if preference_terms:
            pref_limit = int(round(limit * 0.7))
            base_limit = max(limit - pref_limit, 0)
        else:
            pref_limit = 0
            base_limit = limit

        # å…ˆæŠ“å–åŸºç¡€ç»“æœ
        base_items = await self._fetch_by_source(
            source=source,
            search_query=base_search_query,
            limit=max(base_limit, 1 if pref_limit == 0 else 0),
            page=page,
            is_preference=False,
        )

        # å†æŠ“å–åå¥½ç»“æœ
        pref_items: List[Dict] = []
        if pref_limit > 0 and preference_terms:
            pref_items = await self._fetch_preference_items(
                source, base_search_query, preference_terms, pref_limit, page
            )

        # åˆå¹¶å¹¶æŒ‰åå¥½æ‰“åˆ†æ’åº
        merged = self._merge_with_ratio(base_items, pref_items, base_limit, pref_limit, limit)
        scored = self._sort_with_preference_score(merged, preference_terms)

        # å†™å…¥ç¼“å­˜ ä¾›çŸ­æœŸé‡å¤è¯·æ±‚å¤ç”¨
        with self._cache_lock:
            self._cache[cache_key] = (scored, datetime.now())

        return scored[:limit]

    async def _fetch_preference_items(
        self,
        source: str,
        base_search_query: str,
        preference_terms: Sequence[str],
        total_limit: int,
        page: int,
    ) -> List[Dict]:
        """
        æŒ‰åå¥½è¯å¹¶å‘æŠ“å–ç»“æœ

        Args:
            source (str): æ•°æ®æºåç§°
            base_search_query (str): åŸºç¡€æŸ¥è¯¢è¯
            preference_terms (Sequence[str]): åå¥½è¯åˆ—è¡¨
            total_limit (int): æ€»åå¥½ç»“æœä¸Šé™
            page (int): é¡µç 

        Returns:
            List[Dict]: åå¥½ç»“æœåˆ—è¡¨ å·²å»é‡

        Examples:
            >>> # rows = await svc._fetch_preference_items("bilibili" "é­”è£" ["è‰¾ç›"] 20 1)
            >>> # rows is list
            pass
        """
        # æŒ‰åå¥½è¯å‡åˆ†æŠ“å–é…é¢
        per_term = max(1, math.ceil(total_limit / max(len(preference_terms), 1)))
        # æ§åˆ¶å¹¶å‘æ•° é¿å…è¯·æ±‚è¿‡å¯†
        sem = asyncio.Semaphore(3)

        async def _task(term: str) -> List[Dict]:
            async with sem:
                # æ¯ä¸ªåå¥½è¯æ„é€ ç‹¬ç«‹æŸ¥è¯¢
                q = f"{base_search_query} {term}".strip()
                return await self._fetch_by_source(
                    source=source,
                    search_query=q,
                    limit=per_term,
                    page=page,
                    is_preference=True,
                )

        # å¹¶å‘æ‰§è¡Œå…¨éƒ¨åå¥½è¯ä»»åŠ¡
        tasks = [_task(term) for term in preference_terms]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # åˆå¹¶ä»»åŠ¡ç»“æœå¹¶å»é‡
        merged: List[Dict] = []
        seen: Set[str] = set()
        for result in results:
            if isinstance(result, Exception):
                continue
            for item in result:
                uid = self._item_unique_id(item)
                if uid in seen:
                    continue
                seen.add(uid)
                merged.append(item)
        return merged[:total_limit]

    async def _fetch_by_source(
        self,
        source: str,
        search_query: str,
        limit: int,
        page: int,
        is_preference: bool,
    ) -> List[Dict]:
        """
        æŒ‰æ¥æºåˆ†å‘æŠ“å–é€»è¾‘

        Args:
            source (str): æ•°æ®æºåç§°
            search_query (str): æŸ¥è¯¢è¯
            limit (int): ç»“æœä¸Šé™
            page (int): é¡µç 
            is_preference (bool): æ˜¯å¦ä¸ºåå¥½æŠ“å–ç»“æœ

        Returns:
            List[Dict]: ç»Ÿä¸€ç»“æ„ç»“æœåˆ—è¡¨

        Examples:
            >>> # rows = await svc._fetch_by_source("baidu", "é­”è£", 50, 1, False)
        """
        # éæ³•ä¸Šé™ç›´æ¥è¿”å›ç©º
        if limit <= 0:
            return []

        # æ ¹æ®æ¥æºè°ƒç”¨å¯¹åº”æŠ“å–å‡½æ•°
        if source == "bilibili":
            items = await self._fetch_bilibili(search_query, limit, page)
        elif source == "baidu":
            items = await self._fetch_baidu(search_query, limit, page)
        elif source == "google":
            items = await self._fetch_google(search_query, limit)
        else:
            items = []

        # æ ‡æ³¨æ˜¯å¦å‘½ä¸­åå¥½æ£€ç´¢
        for item in items:
            item["is_preference_hit"] = is_preference
        return items

    async def _fetch_bilibili(self, search_query: str, limit: int, page: int = 1) -> List[Dict]:
        """
        æŠ“å– B ç«™æœç´¢ç»“æœ

        Args:
            search_query (str): æŸ¥è¯¢è¯
            limit (int): ç»“æœä¸Šé™
            page (int): èµ·å§‹é¡µç 

        Returns:
            List[Dict]: B ç«™ç»“æœåˆ—è¡¨

        Examples:
            >>> # rows = await svc._fetch_bilibili("é­”è£", 100, 1)
        """
        # è®¾å®šå•é¡µå¤§å°å¹¶ä¼°ç®—é¡µæ•° ä¸Šé™ 5 é¡µ
        page_size = 50
        pages_needed = min(max(math.ceil(limit / page_size), 1), 5)
        page_numbers = list(range(page, page + pages_needed))

        # é™åˆ¶å¹¶å‘ä»¥é™ä½å°ç¦é£é™©
        sem = asyncio.Semaphore(3)

        async def _search_page(p: int) -> List[Dict]:
            async with sem:
                return await self._bilibili_search_page(search_query, p, page_size)

        # å¹¶å‘è¯·æ±‚å¤šä¸ªé¡µç 
        results = await asyncio.gather(*[_search_page(p) for p in page_numbers], return_exceptions=True)

        # è·¨é¡µæŒ‰ bvid å»é‡åˆå¹¶
        merged: List[Dict] = []
        seen_bvid: Set[str] = set()
        for result in results:
            if isinstance(result, Exception):
                continue
            for item in result:
                bvid = item.get("bvid") or ""
                key = bvid or self._item_unique_id(item)
                if key in seen_bvid:
                    continue
                seen_bvid.add(key)
                merged.append(item)

        return merged[:limit]

    async def _bilibili_search_page(self, search_query: str, page: int, page_size: int) -> List[Dict]:
        """
        è°ƒç”¨ B ç«™ä¸»æœç´¢æ¥å£å¹¶è§£æ

        Args:
            search_query (str): æŸ¥è¯¢è¯
            page (int): é¡µç 
            page_size (int): é¡µå¤§å°

        Returns:
            List[Dict]: è§£æåçš„ç»“æœåˆ—è¡¨

        Examples:
            >>> # rows = await svc._bilibili_search_page("é­”è£", 1, 50)
        """
        # ä¸»æœç´¢æ¥å£
        url = "https://api.bilibili.com/x/web-interface/search/type"
        params = {
            "search_type": "video",
            "keyword": search_query,
            "page": page,
            "page_size": page_size,
            "order": "totalrank",
        }
        headers = {
            **self.COMMON_HEADERS,
            "Origin": "https://search.bilibili.com",
            "Referer": "https://search.bilibili.com/",
        }

        # è¯·æ±‚ä¸»æ¥å£ å¤±è´¥åˆ™èµ°å…œåº•æ¥å£
        data = await self._request_json_with_retry(url, params=params, headers=headers, retries=4)
        if not data or data.get("code") != 0:
            # ä¸»æ¥å£å¼‚å¸¸æ—¶å›é€€åˆ° all/v2
            fallback = await self._bilibili_search_page_fallback(search_query, page)
            return fallback

        # è¯»å–ç»“æœæ•°ç»„å¹¶è½¬æ¢ä¸ºç»Ÿä¸€å­—æ®µ
        result_list = data.get("data", {}).get("result", []) or []
        parsed = [self._normalize_bilibili_item(item, search_query) for item in result_list]
        # è¿‡æ»¤è§„èŒƒåŒ–å¤±è´¥çš„ç©ºé¡¹
        return [x for x in parsed if x]

    async def _bilibili_search_page_fallback(self, search_query: str, page: int) -> List[Dict]:
        """
        è°ƒç”¨ B ç«™å…œåº•æœç´¢æ¥å£

        Args:
            search_query (str): æŸ¥è¯¢è¯
            page (int): é¡µç 

        Returns:
            List[Dict]: è§£æåçš„ç»“æœåˆ—è¡¨

        Examples:
            >>> # rows = await svc._bilibili_search_page_fallback("é­”è£", 1)
        """
        # å…œåº•æ¥å£
        url = "https://api.bilibili.com/x/web-interface/search/all/v2"
        params = {
            "keyword": search_query,
            "page": page,
        }
        headers = {
            **self.COMMON_HEADERS,
            "Origin": "https://search.bilibili.com",
            "Referer": "https://search.bilibili.com/",
        }

        data = await self._request_json_with_retry(url, params=params, headers=headers, retries=3)
        if not data or data.get("code") != 0:
            return []

        # all/v2 æ¥å£è¿”å›åˆ†ç»„åˆ—è¡¨ éœ€è¦æŠ½å– video åˆ†ç»„
        result_blocks = data.get("data", {}).get("result", []) or []
        video_block = None
        for block in result_blocks:
            if block.get("result_type") == "video":
                video_block = block
                break
        if not video_block:
            return []

        # å°† video åˆ†ç»„æ•°æ®æ˜ å°„ä¸ºæ ‡å‡†æ¡ç›®
        result_list = video_block.get("data", []) or []
        parsed = [self._normalize_bilibili_item(item, search_query) for item in result_list]
        return [x for x in parsed if x]

    async def _fetch_baidu(self, search_query: str, limit: int, page: int = 1) -> List[Dict]:
        """
        æŠ“å–ç™¾åº¦æœç´¢ç»“æœ

        Args:
            search_query (str): æŸ¥è¯¢è¯
            limit (int): ç»“æœä¸Šé™
            page (int): èµ·å§‹é¡µç 

        Returns:
            List[Dict]: ç™¾åº¦ç»“æœåˆ—è¡¨

        Examples:
            >>> # rows = await svc._fetch_baidu("é­”è£", 50, 1)
            >>> # rows is list
            pass
        """
        # ç™¾åº¦æ¯é¡µæ•°é‡
        rn = 20
        # æœ€å¤šæŠ“å– 6 é¡µ æ§åˆ¶ä¸Šæ¸¸å‹åŠ›
        pages_needed = min(max(math.ceil(limit / rn), 1), 6)
        # ç™¾åº¦åˆ†é¡µå‚æ•°æŒ‰åç§»é‡é€’å¢
        pns = [max(page - 1, 0) * rn + i * rn for i in range(pages_needed)]

        # æ§åˆ¶å¹¶å‘é¿å…è¯·æ±‚è¿‡å¯†
        sem = asyncio.Semaphore(3)

        async def _search_page(pn: int) -> List[Dict]:
            async with sem:
                # å•é¡µè¯·æ±‚å¤ç”¨ç»Ÿä¸€è§£æé€»è¾‘
                return await self._baidu_search_page(search_query, pn, rn)

        results = await asyncio.gather(*[_search_page(pn) for pn in pns], return_exceptions=True)

        merged: List[Dict] = []
        seen: Set[str] = set()
        for result in results:
            if isinstance(result, Exception):
                # å•é¡µå¼‚å¸¸æ—¶å¿½ç•¥ ä¿ç•™å…¶ä»–é¡µç»“æœ
                continue
            for item in result:
                # ä½¿ç”¨ç»Ÿä¸€å”¯ä¸€é”®å»é‡
                key = self._item_unique_id(item)
                if key in seen:
                    continue
                seen.add(key)
                merged.append(item)

        return merged[:limit]

    async def _baidu_search_page(self, search_query: str, pn: int, rn: int) -> List[Dict]:
        """
        æŠ“å–ç™¾åº¦å•é¡µå¹¶è§£æ HTML

        Args:
            search_query (str): æŸ¥è¯¢è¯
            pn (int): åç§»å‚æ•°
            rn (int): å•é¡µæ¡æ•°

        Returns:
            List[Dict]: å•é¡µè§£æç»“æœ

        Examples:
            >>> # rows = await svc._baidu_search_page("é­”è£" 0 20)
            >>> # rows is list
            pass
        """
        # ç™¾åº¦ç½‘é¡µæœç´¢å…¥å£
        url = "https://www.baidu.com/s"
        params = {"wd": search_query, "pn": pn, "rn": rn}

        html = await self._request_text_with_retry(url, params=params, headers=self.COMMON_HEADERS, retries=3)
        if not html:
            return []

        # ä½¿ç”¨ html è§£æå™¨æå–å€™é€‰ç»“æœèŠ‚ç‚¹
        soup = BeautifulSoup(html, "html.parser")
        candidates = soup.select(".result, .result-op, .c-container")

        result: List[Dict] = []
        for item in candidates:
            # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨å…¼å®¹ä¸åŒæ¨¡æ¿
            title_el = item.select_one("h3 a, .t a, .c-title a")
            if not title_el:
                continue

            # æ¸…æ´—æ ‡é¢˜ä¸è·³è½¬é“¾æ¥
            title = title_el.get_text(" ", strip=True)
            link = (title_el.get("href") or "").strip()
            if not title or not link:
                continue

            # æè¿°å­—æ®µåœ¨ä¸åŒæ¨¡ç‰ˆçš„ class ä¸åŒ
            desc_el = item.select_one(".content-right_2s-H4, .c-abstract, .c-span-last")
            desc = desc_el.get_text(" ", strip=True) if desc_el else ""

            # æ„é€ ç»Ÿä¸€æ¡ç›®ç»“æ„ ä¾¿äºå‰ç«¯ç»Ÿä¸€æ¸²æŸ“
            result.append(
                {
                    "id": f"baidu_{hashlib.md5(link.encode('utf-8')).hexdigest()[:12]}",
                    "title": title,
                    "url": link,
                    "source": "baidu",
                    "source_label": "ç™¾åº¦",
                    "thumbnail": "",
                    "date": "",
                    "author": "",
                    "description": desc[:220],
                    "play_count": 0,
                    "danmaku_count": 0,
                    "duration": "",
                    "bvid": "",
                    "search_keyword": search_query,
                    "category": "other",
                    "category_label": CATEGORY_LABELS["other"],
                    "character": "",
                    "character_name": "",
                }
            )
        return result

    async def _fetch_google(self, search_query: str, limit: int) -> List[Dict]:
        """
        æŠ“å– Google News RSS ç»“æœ

        Args:
            search_query (str): æŸ¥è¯¢è¯
            limit (int): ç»“æœä¸Šé™

        Returns:
            List[Dict]: Google ç»“æœåˆ—è¡¨

        Examples:
            >>> # rows = await svc._fetch_google("é­”è£", 30)
            >>> # rows is list
            pass
        """
        # RSS æ£€ç´¢åœ°å€
        url = "https://news.google.com/rss/search"
        params = {
            "q": search_query,
            "hl": "zh-CN",
            "gl": "CN",
            "ceid": "CN:zh-Hans",
        }

        xml_text = await self._request_text_with_retry(url, params=params, headers=self.COMMON_HEADERS, retries=3)
        if not xml_text:
            return []

        # è§£æ RSS xml å¹¶è·å– item èŠ‚ç‚¹
        soup = BeautifulSoup(xml_text, "xml")
        items = soup.find_all("item")

        result: List[Dict] = []
        seen: Set[str] = set()
        for item in items:
            # è¯»å–æ ‡é¢˜ä¸é“¾æ¥ä½œä¸ºæ ¸å¿ƒå­—æ®µ
            title = item.title.text.strip() if item.title and item.title.text else ""
            link = item.link.text.strip() if item.link and item.link.text else ""
            if not title or not link:
                continue
            if link in seen:
                continue
            seen.add(link)

            # å‘å¸ƒæ—¥æœŸç»Ÿä¸€è½¬æ¢ä¸º yyyy-mm-dd
            date_str = ""
            if item.pubDate and item.pubDate.text:
                date_str = self._parse_pub_date(item.pubDate.text)

            # è¯»å–æ¥æºä¸æè¿° ç”¨äºå±•ç¤ºä¸åå¥½æ’åº
            author = item.source.text.strip() if item.source and item.source.text else ""
            desc = item.description.text.strip() if item.description and item.description.text else ""

            # æ˜ å°„ä¸ºç»Ÿä¸€å­—æ®µç»“æ„
            result.append(
                {
                    "id": f"google_{hashlib.md5(link.encode('utf-8')).hexdigest()[:12]}",
                    "title": title,
                    "url": link,
                    "source": "google",
                    "source_label": "Google",
                    "thumbnail": "",
                    "date": date_str,
                    "author": author,
                    "description": desc[:220],
                    "play_count": 0,
                    "danmaku_count": 0,
                    "duration": "",
                    "bvid": "",
                    "search_keyword": search_query,
                    "category": "other",
                    "category_label": CATEGORY_LABELS["other"],
                    "character": "",
                    "character_name": "",
                }
            )

            if len(result) >= limit:
                # åˆ°è¾¾ä¸Šé™åæå‰åœæ­¢éå†
                break

        return result[:limit]

    async def _request_json_with_retry(
        self,
        url: str,
        params: Optional[Dict] = None,
        headers: Optional[Dict] = None,
        retries: int = 3,
    ) -> Optional[Dict]:
        """
        å¸¦é‡è¯•è·å– JSON å“åº”

        Args:
            url (str): è¯·æ±‚åœ°å€
            params (Optional[Dict]): æŸ¥è¯¢å‚æ•°
            headers (Optional[Dict]): è¯·æ±‚å¤´
            retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            Optional[Dict]: JSON å­—å…¸ å¤±è´¥è¿”å› None

        Examples:
            >>> # data = await svc._request_json_with_retry("https://api")
            >>> # data is dict or None
            pass
        """
        # æŒ‡æ•°é€€é¿é‡è¯•
        for attempt in range(1, retries + 1):
            try:
                # æ¯æ¬¡é‡è¯•åˆ›å»ºæ–° client é¿å…è¿æ¥çŠ¶æ€æ±¡æŸ“
                async with httpx.AsyncClient(timeout=15, follow_redirects=True) as client:
                    resp = await client.get(url, params=params, headers=headers)

                if resp.status_code in (412, 429):
                    # è§¦å‘é¢‘æ§æ—¶å¢åŠ ç­‰å¾…æ—¶é—´åé‡è¯•
                    await asyncio.sleep((2 ** (attempt - 1)) + random.random())
                    continue

                if resp.status_code >= 500:
                    # æœåŠ¡ç«¯é”™è¯¯é‡‡ç”¨æŒ‡æ•°é€€é¿
                    await asyncio.sleep((2 ** (attempt - 1)) + random.random() * 0.5)
                    continue

                if resp.status_code != 200:
                    # é 200 ä¸”éå¯é‡è¯•çŠ¶æ€ç›´æ¥è¿”å›å¤±è´¥
                    return None

                # æ­£å¸¸çŠ¶æ€è¿”å› json ç»“æœ
                return resp.json()
            except (httpx.TimeoutException, httpx.NetworkError, httpx.RemoteProtocolError):
                if attempt == retries:
                    return None
                # ç½‘ç»œæŠ–åŠ¨æ—¶ç»§ç»­é‡è¯•
                await asyncio.sleep((2 ** (attempt - 1)) + random.random() * 0.5)
            except Exception:
                # éé¢„æœŸå¼‚å¸¸ä¸é‡è¯• ç›´æ¥å¤±è´¥
                return None
        return None

    async def _request_text_with_retry(
        self,
        url: str,
        params: Optional[Dict] = None,
        headers: Optional[Dict] = None,
        retries: int = 3,
    ) -> Optional[str]:
        """
        å¸¦é‡è¯•è·å–æ–‡æœ¬å“åº”

        Args:
            url (str): è¯·æ±‚åœ°å€
            params (Optional[Dict]): æŸ¥è¯¢å‚æ•°
            headers (Optional[Dict]): è¯·æ±‚å¤´
            retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            Optional[str]: æ–‡æœ¬å†…å®¹ å¤±è´¥è¿”å› None

        Examples:
            >>> # text = await svc._request_text_with_retry("https://www.example.com")
            >>> # text is str or None
            pass
        """
        # æŒ‡æ•°é€€é¿é‡è¯•
        for attempt in range(1, retries + 1):
            try:
                # æ¯æ¬¡é‡è¯•åˆ›å»ºæ–° client ä¿æŒè¯·æ±‚éš”ç¦»
                async with httpx.AsyncClient(timeout=15, follow_redirects=True) as client:
                    resp = await client.get(url, params=params, headers=headers)

                if resp.status_code in (412, 429):
                    # é¢‘æ§å“åº”å»¶è¿Ÿåé‡è¯•
                    await asyncio.sleep((2 ** (attempt - 1)) + random.random())
                    continue

                if resp.status_code >= 500:
                    # æœåŠ¡ç«¯é”™è¯¯é‡è¯•
                    await asyncio.sleep((2 ** (attempt - 1)) + random.random() * 0.5)
                    continue

                if resp.status_code != 200:
                    # ä¸å¯æ¢å¤çŠ¶æ€ç›´æ¥å¤±è´¥
                    return None

                # è¿”å›åŸå§‹æ–‡æœ¬ç»™è§£æå±‚å¤„ç†
                return resp.text
            except (httpx.TimeoutException, httpx.NetworkError, httpx.RemoteProtocolError):
                if attempt == retries:
                    return None
                # è¿æ¥è¶…æ—¶ç­‰å¼‚å¸¸å»¶è¿Ÿé‡è¯•
                await asyncio.sleep((2 ** (attempt - 1)) + random.random() * 0.5)
            except Exception:
                # å…¶ä»–å¼‚å¸¸ç›´æ¥å¤±è´¥
                return None
        return None

    def _normalize_bilibili_item(self, item: Dict, search_query: str) -> Optional[Dict]:
        """
        è§„èŒƒåŒ– B ç«™åŸå§‹ç»“æœé¡¹

        Args:
            item (Dict): åŸå§‹æ¡ç›®å­—å…¸
            search_query (str): æŸ¥è¯¢è¯

        Returns:
            Optional[Dict]: æ ‡å‡†åŒ–ç»“æœå­—å…¸ æ— æ ‡é¢˜æ—¶è¿”å› None

        Examples:
            >>> row = svc._normalize_bilibili_item({"title": "a"}, "é­”è£")
        """
        # æ¸…ç† HTML æ ‡ç­¾åçš„æ ‡é¢˜
        title = re.sub(r"<[^>]+>", "", str(item.get("title") or "")).strip()
        if not title:
            return None

        # è¯»å– bvid ä¸ç¼©ç•¥å›¾å­—æ®µ
        bvid = str(item.get("bvid") or "").strip()
        pic = str(item.get("pic") or item.get("cover") or "").strip()
        if pic.startswith("//"):
            # å…¼å®¹åè®®ç›¸å¯¹åœ°å€
            pic = "https:" + pic

        # æ—¶é—´æˆ³è½¬æ¢ä¸º yyyy-mm-dd
        pubdate = item.get("pubdate") or item.get("pub_time") or 0
        date_str = ""
        try:
            if pubdate:
                date_str = datetime.fromtimestamp(int(pubdate)).strftime("%Y-%m-%d")
        except Exception:
            date_str = ""

        # è§£ææ’­æ”¾é‡ å¼¹å¹•é‡ æ—¶é•¿ ä½œè€…
        play = self._parse_count(item.get("play", 0))
        danmaku = self._parse_count(item.get("video_review", item.get("danmaku", 0)))
        duration = self._normalize_duration(item.get("duration", ""))
        author = str(item.get("author") or item.get("up_name") or "").strip()

        # æ¸…ç†æè¿°ä¸­çš„ HTML æ ‡ç­¾
        description = str(item.get("description") or item.get("desc") or "").strip()
        description = re.sub(r"<[^>]+>", "", description)

        # æ ¹æ®æ ‡é¢˜ä¸æè¿°åŒ¹é…åˆ†ç±»ä¸è§’è‰²
        category, category_label = self._match_category(title, description)
        character, character_name = self._match_character(title, description)

        # ä¼˜å…ˆä½¿ç”¨æ¥å£ç»™å‡ºçš„é“¾æ¥ å¦åˆ™æ ¹æ® bvid æ‹¼æ¥
        url = str(item.get("arcurl") or "").strip()
        if not url and bvid:
            url = f"https://www.bilibili.com/video/{bvid}"

        # è®¡ç®—ç¨³å®š id
        uid_seed = bvid or url or title
        uid = hashlib.md5(uid_seed.encode("utf-8")).hexdigest()[:12]

        # è¿”å›ç»Ÿä¸€ç»“æ„
        return {
            "id": f"bili_{uid}",
            "title": title,
            "url": url,
            "source": "bilibili",
            "source_label": "Bç«™",
            "thumbnail": pic,
            "date": date_str,
            "author": author,
            "description": description[:220],
            "category": category,
            "category_label": category_label,
            "play_count": play,
            "danmaku_count": danmaku,
            "duration": duration,
            "bvid": bvid,
            "search_keyword": search_query,
            "character": character,
            "character_name": character_name,
        }

    def _normalize_duration(self, value) -> str:
        """
        ç»Ÿä¸€æ—¶é•¿å­—æ®µæ ¼å¼

        Args:
            value (Any): åŸå§‹æ—¶é•¿å€¼

        Returns:
            str: è§„èŒƒåŒ–æ—¶é•¿å­—ç¬¦ä¸²

        Examples:
            >>> svc._normalize_duration("1:20")
        """
        # ç©ºå€¼ç›´æ¥è¿”å›ç©ºå­—ç¬¦ä¸²
        if value is None:
            return ""
        if isinstance(value, str):
            text = value.strip()
            if not text:
                return ""
            try:
                # æ”¯æŒ hh:mm:ss mm:ss s ä¸‰ç§æ ¼å¼
                parts = [int(p) for p in text.split(":") if p != ""]
                if len(parts) == 3:
                    h, m, s = parts
                    total_seconds = h * 3600 + m * 60 + s
                elif len(parts) == 2:
                    m, s = parts
                    total_seconds = m * 60 + s
                elif len(parts) == 1:
                    total_seconds = parts[0]
                else:
                    return text
                return self._format_duration(total_seconds)
            except Exception:
                # å­—ç¬¦ä¸²ä¸å¯è§£ææ—¶ä¿ç•™åŸå€¼
                return text
        if isinstance(value, int):
            # æ•´æ•°æŒ‰ç§’å¤„ç†
            return self._format_duration(value)
        # å…¶ä»–ç±»å‹è½¬å­—ç¬¦ä¸²è¿”å›
        return str(value)

    def _format_duration(self, total_seconds: int) -> str:
        """
        å°†ç§’æ•°æ ¼å¼åŒ–ä¸ºæ—¶é•¿æ–‡æœ¬

        Args:
            total_seconds (int): æ€»ç§’æ•°

        Returns:
            str: mm:ss æˆ– hh:mm:ss

        Examples:
            >>> svc._format_duration(65)
            '01:05'
        """
        # é˜²æ­¢è´Ÿå€¼
        total_seconds = max(int(total_seconds), 0)
        if total_seconds <= 3600:
            # å°äºä¸€å°æ—¶è¾“å‡º mm:ss
            m, s = divmod(total_seconds, 60)
            return f"{m:02d}:{s:02d}"
        # è¶…è¿‡ä¸€å°æ—¶è¾“å‡º hh:mm:ss
        m, s = divmod(total_seconds, 60)
        h, m = divmod(m, 60)
        return f"{h:02d}:{m:02d}:{s:02d}"

    def _parse_count(self, raw) -> int:
        """
        è§£ææ’­æ”¾é‡æˆ–å¼¹å¹•é‡æ–‡æœ¬

        Args:
            raw (Any): åŸå§‹æ•°é‡å€¼

        Returns:
            int: è§£æåçš„æ•´æ•°æ•°é‡

        Examples:
            >>> svc._parse_count("1.2ä¸‡")
            12000
        """
        # ç©ºå€¼è¿”å› 0
        if raw is None:
            return 0
        # å»é™¤ç©ºæ ¼ä¸åƒåˆ†ä½ç¬¦å·
        text = str(raw).strip().lower().replace(",", "")
        try:
            if "ä¸‡" in text:
                return int(float(text.replace("ä¸‡", "")) * 10000)
            if text.endswith("w"):
                return int(float(text[:-1]) * 10000)
            return int(float(text))
        except Exception:
            # è§£æå¤±è´¥å›é€€ 0
            return 0

    def _build_cache_key(
        self,
        source: str,
        search_query: str,
        limit: int,
        page: int,
        pref_chars: Sequence[str],
    ) -> str:
        """
        ç”Ÿæˆè¯·æ±‚ç¼“å­˜é”®

        Args:
            source (str): æ•°æ®æº
            search_query (str): æŸ¥è¯¢è¯
            limit (int): ä¸Šé™
            page (int): é¡µç 
            pref_chars (Sequence[str]): åå¥½è§’è‰²åˆ—è¡¨

        Returns:
            str: MD5 ç¼“å­˜é”®

        Examples:
            >>> key = svc._build_cache_key("bilibili", "é­”è£", 50, 1, [])
        """
        # ä½¿ç”¨ç¨³å®š JSON ä¸²ç”Ÿæˆå“ˆå¸Œ
        raw = json.dumps(
            {
                "source": source,
                "search_query": search_query,
                "limit": limit,
                "page": page,
                "default_query": DEFAULT_QUERY,
                "pref_chars": sorted(pref_chars),
            },
            ensure_ascii=False,
            sort_keys=True,
        )
        # è¿”å›å›ºå®šé•¿åº¦ md5 ä½œä¸ºç¼“å­˜é”®
        return hashlib.md5(raw.encode("utf-8")).hexdigest()

    def _normalize_list(self, values: Optional[Sequence[str]]) -> List[str]:
        """
        æ¸…æ´—å­—ç¬¦ä¸²åˆ—è¡¨

        Args:
            values (Optional[Sequence[str]]): åŸå§‹åˆ—è¡¨

        Returns:
            List[str]: å»ç©ºç™½åçš„åˆ—è¡¨

        Examples:
            >>> svc._normalize_list([" a ", ""])
            ['a']
        """
        # ç©ºè¾“å…¥è¿”å›ç©ºåˆ—è¡¨
        if not values:
            return []
        # ä»…ä¿ç•™éç©ºå­—ç¬¦ä¸²å¹¶å»æ‰é¦–å°¾ç©ºç™½
        return [v.strip() for v in values if isinstance(v, str) and v.strip()]

    def _compose_search_query(self, user_query: Optional[str]) -> str:
        """
        ç»„è£…æœ€ç»ˆæ£€ç´¢è¯ é»˜è®¤(é­”æ³•å°‘å¥³çš„é­”å¥³å®¡åˆ¤) + å…¶ä»–

        Args:
            user_query (Optional[str]): ç”¨æˆ·è¾“å…¥è¯

        Returns:
            str: åˆå¹¶åçš„æ£€ç´¢è¯

        Examples:
            >>> svc._compose_search_query("å¸Œç½—")
            'é­”æ³•å°‘å¥³çš„é­”å¥³å®¡åˆ¤ å¸Œç½—'
        """
        # å…ˆæ¸…æ´—ç”¨æˆ·è¾“å…¥
        text = (user_query or "").strip()
        if not text:
            # æ— è¾“å…¥æ—¶ä½¿ç”¨é»˜è®¤æŸ¥è¯¢è¯
            return DEFAULT_QUERY
        if text == DEFAULT_QUERY or text.startswith(f"{DEFAULT_QUERY} "):
            # å·²åŒ…å«é»˜è®¤æŸ¥è¯¢è¯æ—¶ç›´æ¥è¿”å›
            return text
        # å…¶ä»–æƒ…å†µåœ¨å‰é¢è¡¥ä¸Šé»˜è®¤æŸ¥è¯¢è¯
        return f"{DEFAULT_QUERY} {text}"

    def _build_preference_terms(self, preferred_characters: Sequence[str]) -> List[str]:
        """
        æ ¹æ®è§’è‰²åå¥½ç”Ÿæˆåå¥½æ£€ç´¢è¯

        Args:
            preferred_characters (Sequence[str]): è§’è‰²é”®åˆ—è¡¨

        Returns:
            List[str]: å»é‡åçš„ä¸­æ–‡è§’è‰²ååˆ—è¡¨

        Examples:
            >>> terms = svc._build_preference_terms(["Ema"])
        """
        terms: List[str] = []

        # å°†è§’è‰²é”®æ˜ å°„ä¸ºä¸­æ–‡æ˜¾ç¤ºå
        for char_key in preferred_characters:
            char = CHARACTERS.get(char_key)
            if char and char.get("cn"):
                terms.append(char["cn"])

        # å»é‡å¹¶ä¿æŒåŸå§‹é¡ºåº
        out = []
        seen = set()
        for term in terms:
            if term in seen:
                continue
            seen.add(term)
            out.append(term)
        return out

    def _match_category(self, title: str, desc: str) -> Tuple[str, str]:
        """
        æŒ‰å…³é”®è¯åŒ¹é…åˆ†ç±»

        Args:
            title (str): æ ‡é¢˜
            desc (str): æè¿°

        Returns:
            Tuple[str, str]: åˆ†ç±»é”® ä¸ åˆ†ç±»æ ‡ç­¾
        """
        hay = f"{title} {desc}".lower()
        # ä¾æ¬¡åŒ¹é…åˆ†ç±»è§„åˆ™å…³é”®è¯
        for rule in CATEGORY_RULES:
            for kw in rule.get("keywords", []):
                if kw.lower() in hay:
                    cat = rule["category"]
                    return cat, CATEGORY_LABELS.get(cat, CATEGORY_LABELS["other"])
        # æ— å‘½ä¸­åˆ™å½’ç±»ä¸º other
        return "other", CATEGORY_LABELS["other"]

    def _match_character(self, title: str, desc: str) -> Tuple[str, str]:
        """
        åŒ¹é…è§’è‰²åç§°

        Args:
            title (str): æ ‡é¢˜
            desc (str): æè¿°

        Returns:
            Tuple[str, str]: è§’è‰²é”® ä¸ä¸­æ–‡å
        """
        hay = f"{title} {desc}"
        # éå†è§’è‰²é…ç½®è¿›è¡Œåç§°åŒ¹é…
        for key, info in CHARACTERS.items():
            # ç»„åˆä¸­æ–‡å æ—¥æ–‡å ä¸ç®€ç§°
            names = [info.get("cn", ""), info.get("jp", "")] + info.get("short", [])
            if any(name and name in hay for name in names):
                return key, info.get("cn", "")
        # æœªè¯†åˆ«æ—¶è¿”å›ç©ºè§’è‰²
        return "", ""

    def _item_unique_id(self, item: Dict) -> str:
        """
        ç”Ÿæˆæ¡ç›®å”¯ä¸€å“ˆå¸Œ

        Args:
            item (Dict): æ–°é—»æ¡ç›®å­—å…¸

        Returns:
            str: æ¡ç›®å“ˆå¸Œ
        """
        source = item.get("source", "")
        bvid = item.get("bvid", "")
        url = item.get("url", "")
        title = item.get("title", "")
        # ç»„åˆå…³é”®å­—æ®µå½¢æˆç¨³å®šå”¯ä¸€ç§å­
        seed = f"{source}|{bvid}|{url}|{title}"
        # å¯¹ç§å­åš md5 å¾—åˆ°ç¨³å®šçŸ­æ ‡è¯†
        return hashlib.md5(seed.encode("utf-8")).hexdigest()

    def _merge_with_ratio(
        self,
        base_items: List[Dict],
        pref_items: List[Dict],
        base_limit: int,
        pref_limit: int,
        total_limit: int,
    ) -> List[Dict]:
        """
        æŒ‰é…é¢åˆå¹¶åŸºç¡€ç»“æœä¸åå¥½ç»“æœ

        Args:
            base_items (List[Dict]): åŸºç¡€ç»“æœåˆ—è¡¨
            pref_items (List[Dict]): åå¥½ç»“æœåˆ—è¡¨
            base_limit (int): åŸºç¡€é…é¢
            pref_limit (int): åå¥½é…é¢
            total_limit (int): æ€»ä¸Šé™

        Returns:
            List[Dict]: åˆå¹¶å»é‡åçš„ç»“æœ
        """
        merged: List[Dict] = []
        seen: Set[str] = set()

        # å…ˆå¡«å……åå¥½é…é¢
        pref_take = pref_items[:pref_limit] if pref_limit > 0 else []
        for item in pref_take:
            uid = self._item_unique_id(item)
            if uid in seen:
                continue
            seen.add(uid)
            merged.append(item)

        # å†å¡«å……åŸºç¡€é…é¢
        base_take = base_items[:base_limit] if base_limit > 0 else []
        for item in base_take:
            uid = self._item_unique_id(item)
            if uid in seen:
                continue
            seen.add(uid)
            merged.append(item)

        # å¦‚æœä»»ä¸€ä¾§ä¸è¶³åˆ™äº’ç›¸è¡¥é½
        if len(merged) < total_limit:
            for bucket in (pref_items, base_items):
                # ç»§ç»­æŒ‰åŸæœ‰é¡ºåºè¡¥é½å‰©ä½™åé¢
                for item in bucket:
                    uid = self._item_unique_id(item)
                    if uid in seen:
                        continue
                    seen.add(uid)
                    merged.append(item)
                    if len(merged) >= total_limit:
                        break
                if len(merged) >= total_limit:
                    break

        return merged[:total_limit]

    def _sort_with_preference_score(self, items: List[Dict], preference_terms: Sequence[str]) -> List[Dict]:
        """
        æŒ‰åå¥½å‘½ä¸­ä¸çƒ­åº¦æ’åº

        Args:
            items (List[Dict]): æ¡ç›®åˆ—è¡¨
            preference_terms (Sequence[str]): åå¥½è¯åˆ—è¡¨

        Returns:
            List[Dict]: æ’åºåçš„æ¡ç›®åˆ—è¡¨
        """
        if not items:
            return items

        # é¢„å¤„ç†åå¥½è¯ä¸ºå°å†™
        pref_terms = [t.lower() for t in preference_terms]

        def _score(it: Dict) -> Tuple[int, int, str]:
            # æ ‡é¢˜ä¸æè¿°ç»Ÿä¸€å°å†™ååšå‘½ä¸­åˆ¤æ–­
            title = (it.get("title") or "").lower()
            desc = (it.get("description") or "").lower()
            # ä»»ä¸€åå¥½è¯åœ¨æ ‡é¢˜æˆ–æè¿°ä¸­å‡ºç°å³è§†ä¸ºå‘½ä¸­
            hit = any(term in title or term in desc for term in pref_terms)
            # æ¬¡çº§æ’åºä½¿ç”¨æ’­æ”¾é‡
            play = int(it.get("play_count") or 0)
            # æœ«çº§æ’åºä½¿ç”¨æ—¥æœŸå­—ç¬¦ä¸²
            date = it.get("date") or ""
            return (1 if hit else 0, play, date)

        # å‘½ä¸­ä¼˜å…ˆ çƒ­åº¦å…¶æ¬¡ æ—¥æœŸå†æ¬¡
        return sorted(items, key=_score, reverse=True)

    def _parse_pub_date(self, raw: str) -> str:
        """
        è§£æ RSS å‘å¸ƒæ—¶é—´

        Args:
            raw (str): åŸå§‹æ—¥æœŸæ–‡æœ¬

        Returns:
            str: è§„èŒƒæ—¥æœŸå­—ç¬¦ä¸²

        Examples:
            >>> txt = svc._parse_pub_date("Mon, 01 Jan 2024 00:00:00 GMT")
        """
        raw = raw.strip()
        # ä¾æ¬¡å°è¯•å¸¸è§ RSS æ—¥æœŸæ ¼å¼
        for fmt in ("%a, %d %b %Y %H:%M:%S %Z", "%a, %d %b %Y %H:%M:%S %z"):
            try:
                return datetime.strptime(raw, fmt).strftime("%Y-%m-%d")
            except Exception:
                continue
        # å…œåº•è¿”å›å‰ 10 ä½
        return raw[:10]

    def get_sources(self) -> List[Dict]:
        """
        è·å–å¯ç”¨æ¥æºé…ç½®

        Args:
            None:

        Returns:
            List[Dict]: æ¥æºåˆ—è¡¨
        """
        # è¿”å›å‰ç«¯æ¥æºä¸‹æ‹‰é€‰é¡¹
        # icon å­—æ®µç”¨äºæ¥æºæ ‡è¯†å±•ç¤º
        return [
            {"id": "bilibili", "name": "Bç«™", "icon": "ğŸ“º"},
            {"id": "baidu", "name": "ç™¾åº¦", "icon": "ğŸ”µ"},
            {"id": "google", "name": "Google", "icon": "ğŸ”"},
        ]

    def get_categories(self) -> List[Dict]:
        """
        è·å–å¯ç”¨åˆ†ç±»é…ç½®

        Args:
            None:

        Returns:
            List[Dict]: åˆ†ç±»åˆ—è¡¨
        """
        # è¿‡æ»¤ other ä¿æŒåˆ†ç±»é¢æ¿ç®€æ´
        # è¿”å›å€¼æŒ‰ CATEGORY_LABELS å½“å‰é¡ºåºæ„é€ 
        return [{"id": key, "name": value} for key, value in CATEGORY_LABELS.items() if key != "other"]

    def get_characters(self) -> List[Dict]:
        """
        è·å–è§’è‰²é…ç½®åˆ—è¡¨

        Args:
            None:

        Returns:
            List[Dict]: è§’è‰²ä¿¡æ¯åˆ—è¡¨
        """
        # è¾“å‡ºè§’è‰²ä¸»é”® ä¸­æ–‡å ä¸æ—¥æ–‡å
        # è¯¥ç»“æœç”¨äºå‰ç«¯ç­›é€‰ä¸åå¥½é…ç½®
        return [{"id": key, "name": info["cn"], "name_jp": info["jp"]} for key, info in CHARACTERS.items()]
